---
layout: default
title: Challenges  
---

# Challenges  

- ## Ontological Alignment
One of the first and most complex challenges we encountered was aligning the outputs generated by <a href="https://en.wikipedia.org/wiki/Large_language_model">Large Language Models</a> with the strict schema imposed by the  <a href="http://wit.istc.cnr.it/arco/lode/extract?lang=en&url=https://raw.githubusercontent.com/ICCD-MiBACT/ArCo/master/ArCo-release/ontologie/arco/arco.owl">ArCo ontology</a>. <a href="http://wit.istc.cnr.it/arco">ArCo</a>, designed to represent Italian cultural heritage, defines detailed classes and properties with precise domain and range constraints. <a href="https://en.wikipedia.org/wiki/Large_language_model">Large Language Models</a>, however, produce text based on probabilistic patterns and are not inherently aware of these ontological rules. Even small semantic deviations—such as using a property with an incorrect class—could compromise the graph's consistency or cause <a href="https://dati.cultura.gov.it/sparql">SPARQL</a> query failures. To address this, we had to experiment with prompt design, output parsing, and validation mechanisms to ensure both structural and semantic correctness of the generated triples.

- ## Terminological Ambiguity
Specialized terminology in the domain of musical instruments posed a significant challenge. Terms such as “bocchino” (mouthpiece) or “ottone” (brass) may appear straightforward, but their precise meaning—and corresponding ontological role—can shift depending on historical, cultural, or regional context. Mapping these terms to the correct ArCo classes was often non-trivial. <a href="https://en.wikipedia.org/wiki/Large_language_model">Large Language Models</a> frequently generalized or misinterpreted them, resulting in vague or incorrect triples. We addressed this through a combination of automated filtering and manual review, underscoring the need for domain-specific awareness when working with structured data.

- ## Reasoning Complexity (Chain-of-Thought)
Using Chain-of-Thought prompting helped enhance the reasoning capabilities of the models, especially in generating logical sequences of RDF triples or inferring implicit knowledge. However, this approach also introduced challenges. The model’s outputs were often verbose, redundant, or overly detailed, requiring post-processing to extract clean, usable triples. Balancing the richness of natural language explanations with the conciseness and precision required for knowledge graph construction proved to be an ongoing concern.

- ## Data Integration and Deduplication
As the knowledge graph was enriched with new triples, avoiding redundancy and conflicting information became essential. We had to implement entity disambiguation and consistency checks to ensure that each new statement added verified and unique knowledge. Unfortunately, many existing RDF validation tools are not well-suited for semi-structured or automatically generated data, so we developed custom heuristics and relied on manual validation in several cases.

- ## Workflow Complexity
The overall workflow—from <a href="https://dati.cultura.gov.it/sparql">SPARQL</a> querying and prompt generation to triple validation and reintegration—was highly complex. It required tight coordination across different tools, representations, and stages. Errors in one part of the pipeline could easily propagate and impact the rest of the process. To manage this, we adopted a modular and iterative development approach, allowing us to test and refine each component in isolation before full integration.

- ## Conclusion
The challenges faced throughout this project highlighted both the promise and the limitations of combining <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a> with<a href="https://en.wikipedia.org/wiki/Semantic_Web">Semantic Web</a> technologies. While the potential for semi-automated knowledge graph enrichment is substantial, realizing it demands a deep understanding of both the domain and the models involved. Each difficulty encountered became an opportunity to refine our methods and better understand how to build robust, reliable, and semantically accurate systems for future work.

